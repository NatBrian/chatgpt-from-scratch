{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Modern Large Language Model from Scratch\n",
        "\n",
        "**Objective:** Construct a Generative Pre-trained Transformer (GPT) with modern architectural components (Llama 3 style) and train it on a high-quality dataset.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "| Stage | Topic | Time |\n",
        "|-------|-------|------|\n",
        "| 0 | Setup | 5 min |\n",
        "| 1 | Bigram Model (Simplest LM) | 15 min |\n",
        "| 2 | Tokenization | 15 min |\n",
        "| 3 | Attention (4 Versions) | 30 min |\n",
        "| 4 | Modern Components | 20 min |\n",
        "| 5 | Full GPT Model | 15 min |\n",
        "| 6 | Training | 25 min |\n",
        "| 7 | Inference & Chat | 10 min |\n",
        "| 8 | RLHF Alignment | 15 min |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive for checkpoints\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/nanochat_zero\"\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "print(f\"Project: {PROJECT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch tiktoken datasets matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math, time\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: The Simplest Language Model (Bigram)\n",
        "\n",
        "Before Transformers, let's understand the core idea with the SIMPLEST possible model.\n",
        "\n",
        "### What is Language Modeling?\n",
        "Predict the next token: \"The cat sat on the ___\" â†’ \"mat\"\n",
        "\n",
        "### Bigram Model\n",
        "Only looks at the LAST token to predict. No context at all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download tiny dataset for fast iteration\n",
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "print(f\"Dataset: {len(text):,} characters\")\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character-level tokenizer (simplest possible)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "print(f\"Vocab: {vocab_size} chars\")\n",
        "print(f\"'hello' -> {encode('hello')} -> {decode(encode('hello'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "\n",
        "block_size, batch_size = 8, 32\n",
        "def get_batch(split):\n",
        "    d = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input/Target Relationship\n",
        "```\n",
        "Input:  [H, e, l, l, o]\n",
        "Target: [e, l, l, o, !]\n",
        "```\n",
        "Model learns: \"After H, predict e. After e, predict l.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, vocab_size)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.embed(idx)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new):\n",
        "        for _ in range(max_new):\n",
        "            logits, _ = self(idx)\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            idx = torch.cat((idx, torch.multinomial(probs, 1)), dim=1)\n",
        "        return idx\n",
        "\n",
        "bigram = BigramModel(vocab_size).to(device)\n",
        "print(f\"Params: {sum(p.numel() for p in bigram.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Before training - random garbage\n",
        "print(\"BEFORE training:\")\n",
        "print(decode(bigram.generate(torch.zeros((1,1), dtype=torch.long, device=device), 100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train bigram\n",
        "opt = torch.optim.AdamW(bigram.parameters(), lr=1e-3)\n",
        "for step in range(1000):\n",
        "    xb, yb = get_batch('train')\n",
        "    _, loss = bigram(xb, yb)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "    if step % 200 == 0: print(f\"Step {step}: loss={loss.item():.4f}\")\n",
        "print(f\"Final: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After training - slightly better garbage\n",
        "print(\"AFTER training:\")\n",
        "print(decode(bigram.generate(torch.zeros((1,1), dtype=torch.long, device=device), 200)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 1\n",
        "\n",
        "**Learned:** Training loop (forward â†’ loss â†’ backward â†’ update)\n",
        "\n",
        "**Problem:** Bigram only sees the LAST token. In \"The cat sat\", it only sees \"t\" from \"sat\".\n",
        "\n",
        "**Solution:** ATTENTION - look at ALL previous tokens!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Tokenization\n",
        "\n",
        "### Character vs Subword\n",
        "| Text | Chars | BPE (GPT) |\n",
        "|------|-------|-----------|\n",
        "| \"Hello\" | 5 | 1 |\n",
        "| \"The quick fox\" | 13 | 4 |\n",
        "\n",
        "BPE compresses common patterns into single tokens.\n",
        "\n",
        "### BPE Intuition\n",
        "Start with characters, merge frequent pairs:\n",
        "```\n",
        "['H','e','l','l','o'] â†’ ['He','l','l','o'] â†’ ['Hell','o'] â†’ ['Hello']\n",
        "```\n",
        "\n",
        "Building BPE from scratch = ~150 lines. We'll use tiktoken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "text = \"Hello, I am learning AI!\"\n",
        "tokens = enc.encode(text)\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"{len(text)} chars -> {len(tokens)} tokens ({len(text)/len(tokens):.1f}x compression)\")\n",
        "\n",
        "# Decode each\n",
        "for t in tokens:\n",
        "    print(f\"  {t} -> {repr(enc.decode([t]))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizer wrapper\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        self.vocab_size = 50304  # Padded for efficiency\n",
        "    \n",
        "    def encode(self, text):\n",
        "        return torch.tensor(self.enc.encode(text), dtype=torch.long)\n",
        "    \n",
        "    def decode(self, tokens):\n",
        "        if isinstance(tokens, torch.Tensor): tokens = tokens.tolist()\n",
        "        return self.enc.decode(tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 2\n",
        "\n",
        "**Learned:** BPE merges frequent patterns for compression.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 3: Attention (The Heart of Transformers)\n",
        "\n",
        "Goal: For each position, aggregate info from ALL previous positions.\n",
        "\n",
        "We'll build this in 4 versions, each adding one insight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 1: Naive Averaging (For Loop)\n",
        "For position t, average all positions 0...t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "B, T, C = 4, 8, 2  # batch, time, channels\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Slow but clear\n",
        "xbow = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xbow[b, t] = x[b, :t+1].mean(dim=0)\n",
        "\n",
        "print(\"Row 0 = just x[0,0]\")\n",
        "print(\"Row 1 = avg(x[0,0:2])\")\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 2: Matrix Multiplication Trick\n",
        "Lower-triangular matrix does the same thing, but batched!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print(\"Weight matrix:\")\n",
        "print(wei)\n",
        "xbow2 = wei @ x\n",
        "print(f\"\\nSame result? {torch.allclose(xbow, xbow2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 3: Softmax for Learnable Weights\n",
        "Use softmax to convert scores to probabilities.\n",
        "Mask future with -inf (becomes 0 after softmax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei)\n",
        "xbow3 = wei @ x\n",
        "print(f\"Same? {torch.allclose(xbow, xbow3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 4: Self-Attention\n",
        "Instead of fixed weights, LEARN what to attend to!\n",
        "\n",
        "- **Query (Q):** \"What am I looking for?\"\n",
        "- **Key (K):** \"What do I contain?\"\n",
        "- **Value (V):** \"What information do I provide?\"\n",
        "\n",
        "Attention weight = how well Q matches K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k, q, v = key(x), query(x), value(x)\n",
        "\n",
        "# Attention scores\n",
        "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5)  # Scale!\n",
        "wei = wei.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ v\n",
        "\n",
        "print(\"Attention weights (learned!):\")\n",
        "print(wei[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Attention Patterns\n",
        "\n",
        "Let's **see** what the attention weights actually look like!\n",
        "\n",
        "The heatmap shows: **\"How much does position i attend to position j?\"**\n",
        "- **Rows** = query positions (current token asking \"what should I look at?\")\n",
        "- **Columns** = key positions (tokens being looked at)\n",
        "- **Brighter colors** = higher attention weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize attention weights for batch 0\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Attention heatmap\n",
        "ax1 = axes[0]\n",
        "attn_weights = wei[0].detach().numpy()  # Shape: (T, T)\n",
        "im1 = ax1.imshow(attn_weights, cmap='Blues', aspect='auto')\n",
        "ax1.set_xlabel('Key Position (tokens being attended to)', fontsize=10)\n",
        "ax1.set_ylabel('Query Position (current token)', fontsize=10)\n",
        "ax1.set_title('Attention Heatmap\\n(Causal Mask: can only see past + current)', fontsize=11)\n",
        "ax1.set_xticks(range(T))\n",
        "ax1.set_yticks(range(T))\n",
        "plt.colorbar(im1, ax=ax1, label='Attention Weight')\n",
        "\n",
        "# Add visual markers for masked positions\n",
        "for i in range(T):\n",
        "    for j in range(T):\n",
        "        if j > i:  # Masked (future) positions\n",
        "            ax1.text(j, i, 'X', ha='center', va='center', color='red', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 2: Line plot showing attention distribution per position\n",
        "ax2 = axes[1]\n",
        "colors = plt.cm.viridis([0.2, 0.4, 0.6, 0.8])\n",
        "positions_to_show = [0, 2, 5, 7]\n",
        "for idx, pos in enumerate(positions_to_show):\n",
        "    ax2.plot(range(T), attn_weights[pos, :], 'o-', color=colors[idx], \n",
        "             label=f'Position {pos}', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Key Position (tokens being attended to)', fontsize=10)\n",
        "ax2.set_ylabel('Attention Weight', fontsize=10)\n",
        "ax2.set_title('Attention Distribution per Query Position', fontsize=11)\n",
        "ax2.legend(loc='upper right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(range(T))\n",
        "ax2.set_ylim(-0.05, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key observations:\")\n",
        "print(\"1. Lower-triangular pattern: Can only attend to past + current (causal mask)\")\n",
        "print(\"2. Position 0 attends ONLY to itself (weight = 1.0)\")\n",
        "print(\"3. Later positions can distribute attention across more tokens\")\n",
        "print(\"4. Each row sums to 1.0 (softmax normalization)\")\n",
        "print(f\"\\nRow sums: {[f'{s:.2f}' for s in attn_weights.sum(axis=1)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Scale by âˆšd?\n",
        "Without scaling, large dot products â†’ peaky softmax â†’ vanishing gradients.\n",
        "\n",
        "Let's visualize this problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the scaling problem visually\n",
        "q_demo, k_demo = torch.randn(8, 64), torch.randn(8, 64)\n",
        "raw = q_demo @ k_demo.T\n",
        "scaled = raw * (64 ** -0.5)\n",
        "\n",
        "print(f\"Raw variance: {raw.var():.1f} (grows with dimension!)\")\n",
        "print(f\"Scaled variance: {scaled.var():.1f} (stable around 1.0)\")\n",
        "print()\n",
        "\n",
        "# Visualize the effect on softmax\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Without scaling\n",
        "raw_probs = F.softmax(raw, dim=-1).detach().numpy()\n",
        "im1 = axes[0].imshow(raw_probs, cmap='hot', aspect='auto')\n",
        "axes[0].set_title('WITHOUT Scaling\\n(Peaky - almost one-hot!)', fontsize=11)\n",
        "axes[0].set_xlabel('Key Position')\n",
        "axes[0].set_ylabel('Query Position')\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# With scaling\n",
        "scaled_probs = F.softmax(scaled, dim=-1).detach().numpy()\n",
        "im2 = axes[1].imshow(scaled_probs, cmap='hot', aspect='auto')\n",
        "axes[1].set_title('WITH Scaling (divide by sqrt(d))\\n(Diffuse - can learn!)', fontsize=11)\n",
        "axes[1].set_xlabel('Key Position')\n",
        "axes[1].set_ylabel('Query Position')\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Without scaling: Network is 'overconfident' before learning!\")\n",
        "print(\"With scaling: Attention is diffuse, allowing gradients to flow and learn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 3\n",
        "\n",
        "**4 Versions of Attention:**\n",
        "1. For-loop averaging (slow but clear)\n",
        "2. Matrix multiply (fast, batched)\n",
        "3. Softmax + mask (differentiable, causal)\n",
        "4. Q, K, V (learned, data-dependent)\n",
        "\n",
        "**Key insight:** Self-attention = learned weighted average!\n",
        "\n",
        "**Visualization showed:**\n",
        "- Causal mask creates lower-triangular attention pattern\n",
        "- Scaling prevents softmax from becoming one-hot\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 4: Modern Components\n",
        "\n",
        "### 4.1 RMSNorm\n",
        "LayerNorm: subtract mean, divide by std (2 stats)\n",
        "RMSNorm: divide by RMS only (1 stat, 10-15% faster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * rms * self.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 RoPE (Rotary Position Embeddings)\n",
        "Old: Add position vector (position info can get drowned)\n",
        "New: Rotate embeddings (relative position = angle difference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precompute_freqs_cis(dim, max_len, theta=10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
        "    t = torch.arange(max_len)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    # xq, xk: (B, T, nh, head_dim)\n",
        "    # freqs_cis: (T, head_dim//2) complex\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    # Broadcast freqs_cis to (1, T, 1, head_dim//2) for multi-head\n",
        "    freqs_cis = freqs_cis[:xq.shape[1]].unsqueeze(0).unsqueeze(2)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 SwiGLU\n",
        "Old: Linear â†’ ReLU â†’ Linear\n",
        "New: Linear â†’ SiLU Ã— Gate â†’ Linear (more expressive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim, hidden=None, bias=False):\n",
        "        super().__init__()\n",
        "        hidden = hidden or int(dim * 4 * 2/3)\n",
        "        self.w1 = nn.Linear(dim, hidden, bias=bias)\n",
        "        self.w2 = nn.Linear(hidden, dim, bias=bias)\n",
        "        self.w3 = nn.Linear(dim, hidden, bias=bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 4\n",
        "**Modern upgrades:** RMSNorm, RoPE, SwiGLU\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 5: Full GPT Model\n",
        "\n",
        "Now we assemble all components!\n",
        "\n",
        "```\n",
        "Tokens â†’ Embed â†’ [Block Ã— N] â†’ Norm â†’ Logits\n",
        "Block = Attention + FFN (with residuals)\n",
        "```\n",
        "\n",
        "### Configuration Presets\n",
        "\n",
        "| Mode | Layers | Embed | Params | Memory | Best For |\n",
        "|------|--------|-------|--------|--------|----------|\n",
        "| **COLAB_MODE=True** | 6 | 384 | ~30M | ~2GB | Free Colab T4 |\n",
        "| COLAB_MODE=False | 12 | 768 | ~124M | ~6GB | Better GPU |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "# Toggle for Colab Free Tier compatibility\n",
        "COLAB_MODE = True  # Set False if you have a better GPU (A100, H100, RTX 4090)\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # For Colab T4: smaller model that fits in 16GB VRAM\n",
        "    # For better GPU: larger model for better quality\n",
        "    block_size: int = 512 if COLAB_MODE else 1024\n",
        "    vocab_size: int = 50304  # GPT-2 vocab, padded for efficiency\n",
        "    n_layer: int = 6 if COLAB_MODE else 12\n",
        "    n_head: int = 6 if COLAB_MODE else 12\n",
        "    n_embd: int = 384 if COLAB_MODE else 768\n",
        "    dropout: float = 0.1 if COLAB_MODE else 0.0  # Regularization helps small models\n",
        "    bias: bool = False\n",
        "\n",
        "config = GPTConfig()\n",
        "print(f\"Config: {config.n_layer}L, {config.n_embd}d, context={config.block_size}\")\n",
        "print(f\"Mode: {'Colab-Friendly (30M params)' if COLAB_MODE else 'Full (124M params)'}\")\n",
        "print(f\"Estimated memory: {'~2GB' if COLAB_MODE else '~6GB'}\")\n",
        "print()\n",
        "print(\"To use full config, set COLAB_MODE = False above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        \n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "    \n",
        "    def forward(self, x, freqs_cis=None):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        if freqs_cis is not None:\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "            q, k = apply_rotary_emb(q, k, freqs_cis)\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "        \n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "            att = att.masked_fill(torch.tril(torch.ones(T, T, device=x.device)) == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            y = att @ v\n",
        "        \n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.dropout(self.c_proj(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = RMSNorm(config.n_embd)\n",
        "        self.mlp = SwiGLU(config.n_embd)\n",
        "    \n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.attn(self.ln_1(x), freqs_cis)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = RMSNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        \n",
        "        # Weight tying: share embedding weights with output layer\n",
        "        # Why? The \"meaning\" of token embeddings and predictions should be consistent\n",
        "        # Bonus: Saves ~19M parameters (vocab_size Ã— n_embd = 50304 Ã— 384 = 19.3M)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        \n",
        "        self.freqs_cis = precompute_freqs_cis(\n",
        "            config.n_embd // config.n_head, config.block_size * 2\n",
        "        ).to(device)\n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None: torch.nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        x = self.transformer.wte(idx)\n",
        "        freqs_cis = self.freqs_cis[:T]\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, freqs_cis)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        \n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "        return logits, loss\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx = torch.cat((idx, torch.multinomial(probs, 1)), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPT(config).to(device)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
        "\n",
        "# Apply torch.compile for ~1.5-2x speedup (PyTorch 2.0+)\n",
        "if hasattr(torch, 'compile'):\n",
        "    print(\"Compiling model with torch.compile...\")\n",
        "    model = torch.compile(model)\n",
        "    print(\"Model compiled successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 5\n",
        "**Built:** Full GPT with RMSNorm, RoPE, SwiGLU, Flash Attention, torch.compile\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 6: Training\n",
        "\n",
        "Using FineWeb-Edu (high-quality educational text).\n",
        "\n",
        "### Training Features\n",
        "- **Mixed Precision**: float16 for 2x memory savings\n",
        "- **Gradient Accumulation**: Larger effective batch size\n",
        "- **LR Warmup + Cosine Decay**: Standard best practice\n",
        "- **Checkpointing**: Resume training across Colab sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What to Expect During Training\n",
        "\n",
        "| Steps | Loss | Text Quality |\n",
        "|-------|------|--------------|\n",
        "| 0 | ~10 | Random noise |\n",
        "| 500 | ~6-7 | Some words recognizable |\n",
        "| 2000 | ~5 | Sentences form (broken grammar) |\n",
        "| 5000+ | ~4 | Coherent paragraphs |\n",
        "\n",
        "> **Note:** With 6 layers and 512 context, this is a \"baby\" model.\n",
        "> Real ChatGPT has 100+ layers and 100K+ context.\n",
        "> The goal here is **understanding**, not production quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### If You Get \"CUDA Out of Memory\"\n",
        "\n",
        "1. **Reduce batch_size:** Change `batch_size = 4` â†’ `batch_size = 2` or `1`\n",
        "2. **Reduce layers:** Set `COLAB_MODE = True` above (uses 6 layers)\n",
        "3. **Restart runtime:** Runtime â†’ Restart runtime\n",
        "4. **Clear GPU cache:** Run this cell:\n",
        "\n",
        "```python\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "```\n",
        "\n",
        "5. **Reduce context:** Change `block_size` from 512 to 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, batch_size, block_size, split='train'):\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \n",
        "            name=\"sample-10BT\", split=split, streaming=True)\n",
        "        self.iterator = iter(self.dataset)\n",
        "        self.buffer = []\n",
        "    \n",
        "    def __iter__(self): return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        needed = self.batch_size * self.block_size + 1\n",
        "        while len(self.buffer) < needed:\n",
        "            try:\n",
        "                text = next(self.iterator)['text']\n",
        "                self.buffer.extend(tokenizer.enc.encode(text))\n",
        "            except StopIteration:\n",
        "                self.iterator = iter(self.dataset)\n",
        "        \n",
        "        chunk = self.buffer[:needed]\n",
        "        self.buffer = self.buffer[needed:]\n",
        "        data = torch.tensor(chunk, dtype=torch.long)\n",
        "        x = data[:-1].view(self.batch_size, self.block_size)\n",
        "        y = data[1:].view(self.batch_size, self.block_size)\n",
        "        return x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "max_iters = 5000              # P1: Increased from 1000 for meaningful training\n",
        "warmup_iters = 200            # P1: LR warmup for stability\n",
        "eval_interval = 250           # Show progress every 250 steps\n",
        "save_interval = 500           # Checkpoint every 500 steps (important for Colab!)\n",
        "batch_size = 4 if COLAB_MODE else 8  # Smaller batch for Colab\n",
        "grad_accum_steps = 4          # P2: Effective batch = batch_size Ã— 4 = 16 or 32\n",
        "max_lr = 3e-4\n",
        "min_lr = 1e-5\n",
        "\n",
        "# Learning rate schedule: warmup + cosine decay\n",
        "def get_lr(it):\n",
        "    # Linear warmup\n",
        "    if it < warmup_iters:\n",
        "        return max_lr * (it + 1) / warmup_iters\n",
        "    # Cosine decay after warmup\n",
        "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "print(f\"Training config:\")\n",
        "print(f\"  - Max iterations: {max_iters:,}\")\n",
        "print(f\"  - Batch size: {batch_size} Ã— {grad_accum_steps} = {batch_size * grad_accum_steps} effective\")\n",
        "print(f\"  - Tokens per step: {batch_size * grad_accum_steps * config.block_size:,}\")\n",
        "print(f\"  - LR: {min_lr} â†’ {max_lr} â†’ {min_lr} (warmup + cosine)\")\n",
        "print(f\"  - Checkpointing every {save_interval} steps to Google Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize training components\n",
        "train_loader = DataLoader(batch_size, config.block_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "scaler = torch.amp.GradScaler('cuda')  # For mixed precision training\n",
        "\n",
        "# Resume if checkpoint exists\n",
        "CKPT = os.path.join(PROJECT_DIR, \"ckpt.pt\")\n",
        "start_iter = 0\n",
        "if os.path.exists(CKPT):\n",
        "    ckpt = torch.load(CKPT, map_location=device, weights_only=False)  # Contains config dict\n",
        "    # Handle compiled model state dict\n",
        "    state_dict = ckpt['model']\n",
        "    # Remove '_orig_mod.' prefix if present (from torch.compile)\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        new_key = k.replace('_orig_mod.', '')\n",
        "        new_state_dict[new_key] = v\n",
        "    model.load_state_dict(new_state_dict, strict=False)\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    start_iter = ckpt['iter'] + 1\n",
        "    print(f\"Resumed from step {start_iter}\")\n",
        "else:\n",
        "    print(\"Starting fresh training\")\n",
        "\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with gradient accumulation\n",
        "print(\"Starting training loop...\")\n",
        "print(f\"Will train for {max_iters:,} iterations\")\n",
        "print(f\"Estimated time on T4: {max_iters * 0.3 / 60:.1f} minutes\")\n",
        "print()\n",
        "\n",
        "model.train()\n",
        "t0 = time.time()\n",
        "running_loss = 0.0\n",
        "\n",
        "for it in range(start_iter, max_iters):\n",
        "    # Update learning rate (P1: LR warmup + cosine decay)\n",
        "    lr = get_lr(it)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    \n",
        "    # Gradient accumulation loop (P2)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        xb, yb = next(train_loader)\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
        "            logits, loss = model(xb, yb)\n",
        "            loss = loss / grad_accum_steps  # Scale loss for accumulation\n",
        "        scaler.scale(loss).backward()\n",
        "    \n",
        "    # Gradient clipping for stability\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    \n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    \n",
        "    running_loss += loss.item() * grad_accum_steps\n",
        "    \n",
        "    # Logging\n",
        "    if it % 10 == 0:\n",
        "        avg_loss = running_loss / 10 if it > 0 else running_loss\n",
        "        losses.append(avg_loss)\n",
        "        running_loss = 0.0\n",
        "        dt = time.time() - t0; t0 = time.time()\n",
        "        print(f\"Step {it:5d}/{max_iters} | loss={avg_loss:.4f} | lr={lr:.2e} | {dt*1000:.0f}ms\")\n",
        "    \n",
        "    # Visualization\n",
        "    if it % eval_interval == 0 and it > 0:\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('Step (x10)')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Training Progress - Step {it}/{max_iters}')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    \n",
        "    # Checkpointing to Google Drive (critical for Colab!)\n",
        "    if it % save_interval == 0 and it > 0:\n",
        "        # Get raw model state dict (handle torch.compile)\n",
        "        raw_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "        torch.save({\n",
        "            'model': raw_model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'iter': it,\n",
        "            'config': config,\n",
        "            'losses': losses\n",
        "        }, CKPT)\n",
        "        print(f\"ðŸ’¾ Checkpoint saved to Google Drive (step {it})\")\n",
        "\n",
        "print()\n",
        "print(\"âœ… Training complete!\")\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"Checkpoint saved to: {CKPT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 6\n",
        "**Training features:**\n",
        "- Mixed precision (float16) for 2x memory savings\n",
        "- Gradient accumulation for larger effective batch\n",
        "- LR warmup + cosine decay for stable training\n",
        "- Google Drive checkpointing for Colab session recovery\n",
        "- Gradient clipping for stability\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 7: Inference & Chat\n",
        "\n",
        "Now talk to your model!\n",
        "Note: This is a BASE model (completion), not an assistant (yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(max_tokens=100, temp=0.8):\n",
        "    model.eval()  # Switch to evaluation mode\n",
        "    print(\"ðŸ’¬ Chat (type 'exit' to stop)\")\n",
        "    print(\"-\" * 40)\n",
        "    while True:\n",
        "        prompt = input(\"You: \")\n",
        "        if prompt.lower() == 'exit': break\n",
        "        \n",
        "        idx = tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
        "        print(\"AI: \", end=\"\", flush=True)\n",
        "        \n",
        "        for _ in range(max_tokens):\n",
        "            out = model.generate(idx, max_new=1, temperature=temp)\n",
        "            new_tok = out[0, -1].item()\n",
        "            print(tokenizer.decode([new_tok]), end=\"\", flush=True)\n",
        "            idx = out\n",
        "        print(\"\\n\" + \"-\" * 40)\n",
        "\n",
        "# Uncomment to start interactive chat:\n",
        "# chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 7\n",
        "**Built:** Interactive chat with streaming output\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 8: RLHF Alignment\n",
        "\n",
        "**Goal:** Teach the model specific behaviors using Reinforcement Learning.\n",
        "\n",
        "**Simple example:** Make it \"positive\" (reward words like happy, good, great)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_reward(text):\n",
        "    \"\"\"Simple reward: +1 for each positive word\"\"\"\n",
        "    positive = [\"happy\", \"good\", \"great\", \"excellent\", \"love\", \"wonderful\", \"amazing\"]\n",
        "    return sum(1 for w in positive if w in text.lower())\n",
        "\n",
        "# RLHF Demonstration - CONCEPTUAL ONLY (no learning occurs!)\n",
        "# Missing for true RLHF: log_probs collection, value function, PPO loss, gradient update\n",
        "# This demo only shows: sampling/scoring loop (the reward signal concept)\n",
        "print(\"Initializing RLHF demonstration...\")\n",
        "prompts = [\"Today I feel\", \"The weather is\", \"My work is\"]\n",
        "\n",
        "for i in range(50):\n",
        "    prompt = prompts[i % len(prompts)]\n",
        "    idx = tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Generate 4 samples (in real RLHF, we'd track log_probs here)\n",
        "    responses = []\n",
        "    for _ in range(4):\n",
        "        out = model.generate(idx, max_new=10, temperature=0.9)\n",
        "        responses.append(tokenizer.decode(out[0].tolist()))\n",
        "    \n",
        "    # Score responses\n",
        "    rewards = [get_reward(r) for r in responses]\n",
        "    avg_reward = sum(rewards) / len(rewards)\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(f\"Step {i}: Avg reward = {avg_reward:.2f}\")\n",
        "        print(f\"  Best: {responses[rewards.index(max(rewards))]}\")\n",
        "\n",
        "print(\"RLHF demonstration complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 8\n",
        "\n",
        "**RLHF basics:**\n",
        "1. Generate multiple samples\n",
        "2. Score with reward function\n",
        "3. Encourage high-scoring outputs\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "You have successfully implemented a complete GPT architecture with modern enhancements.\n",
        "\n",
        "**Key Concepts Covered:**\n",
        "- Byte Pair Encoding (BPE)\n",
        "- Causal Self-Attention and Multi-Head Attention\n",
        "- RMSNorm, RoPE, and SwiGLU\n",
        "- Mixed-precision training and gradient accumulation\n",
        "\n",
        "**Next steps:**\n",
        "1. Train longer (100k+ steps)\n",
        "2. Use larger/better datasets\n",
        "3. Fine-tune on dialogue for assistant behavior"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
