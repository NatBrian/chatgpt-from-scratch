{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Modern Large Language Model from Scratch\n",
        "\n",
        "**Objective:** We will build a Generative Pre-trained Transformer (GPT) from scratch. This is the same technology behind ChatGPT, Claude, and Llama.\n",
        "\n",
        "**No prior knowledge required.** We will explain every concept as if you are new to AI.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "| Stage | Topic | What we are building |\n",
        "|-------|-------|----------------------|\n",
        "| 0 | Setup | Preparing our tools |\n",
        "| 1 | Bigram Model | A model that guesses the next word based on just one word |\n",
        "| 2 | Tokenization | Converting words into numbers the computer understands |\n",
        "| 3 | Attention | Giving the model 'memory' to look at previous words |\n",
        "| 4 | Modern Components | Upgrading the engine with 2024 technology (Llama 3) |\n",
        "| 5 | Full GPT Model | Assembling the robot |\n",
        "| 6 | Training | Teaching the robot to read and write |\n",
        "| 7 | Inference & Chat | Talking to our creation |\n",
        "| 8 | RLHF Alignment | Teaching the robot manners |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 0: Setup\n",
        "\n",
        "Before we build, we need tools. We will use **PyTorch**, which is a library that helps us do the heavy math required for AI.\n",
        "\n",
        "We also need a **GPU** (Graphics Processing Unit). Think of a CPU (your computer's main brain) as a math professor—very smart but does one problem at a time. A GPU is like a thousand elementary school students—they can solve thousands of simple problems at the exact same time. AI needs this parallel speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup local checkpoint directory\n",
        "# Since we are not using Google Drive, we will save files locally.\n",
        "# WARNING: These files will be DELETED if the Colab runtime disconnects.\n",
        "import os\n",
        "PROJECT_DIR = \"checkpoints\"\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "print(f\"Local checkpoint directory created at: {PROJECT_DIR}\")\n",
        "print(\"Remember to DOWNLOAD your checkpoint file ('ckpt.pt') to your computer to save your progress!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the necessary libraries.\n",
        "# 'torch': The framework for building neural networks.\n",
        "# 'tiktoken': A tool from OpenAI to turn text into numbers.\n",
        "# 'datasets': A library to download books and text for training.\n",
        "!pip install -q torch tiktoken datasets matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math, time\n",
        "\n",
        "# Check if we have a GPU available.\n",
        "# 'cuda' means we have an NVIDIA GPU. 'cpu' means we are using the slow processor.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: The Simplest Language Model (Bigram)\n",
        "\n",
        "### What is a Language Model?\n",
        "A language model is just a machine that plays a guessing game. You give it a sequence of words, and it guesses what comes next.\n",
        "\n",
        "**Example:** \"The cat sat on the _____.\"\n",
        "You probably guessed \"mat\" or \"floor\". That's because you have a language model in your brain.\n",
        "\n",
        "### The Bigram Model\n",
        "We will start with the simplest possible version: a **Bigram Model**.\n",
        "This model is very forgetful. It looks **only at the very last word** to guess the next one. It ignores everything else.\n",
        "\n",
        "- Input: \"The cat sat on the\"\n",
        "- Bigram sees only: \"the\"\n",
        "- Bigram guesses: \"end\" (because \"the end\" is common)\n",
        "\n",
        "It has no memory of the \"cat\" or the \"sitting\". Let's build it to see how bad it is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download a small dataset (Shakespeare) to practice on.\n",
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "print(f\"Dataset length: {len(text):,} characters\")\n",
        "print(\"First 200 characters:\")\n",
        "print(text[:200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Computers cannot understand letters like 'a' or 'b'. They only understand numbers.\n",
        "# We need a 'Tokenizer' to convert characters into integers.\n",
        "\n",
        "# 1. Find all unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# 2. Create a map from character to integer (stoi) and integer to character (itos)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# 3. Define helper functions to convert back and forth\n",
        "encode = lambda s: [stoi[c] for c in s] # Text -> Numbers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Numbers -> Text\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size} unique characters\")\n",
        "print(f\"Example: 'hello' becomes {encode('hello')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the entire text into a long list of numbers (tensor)\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Split into training (90%) and validation (10%) sets.\n",
        "# We train on the 90% and test on the 10% to make sure the model isn't just memorizing.\n",
        "n = int(0.9 * len(data))\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "\n",
        "block_size = 8   # The maximum number of characters we look at (context)\n",
        "batch_size = 32  # How many independent sequences we process at once (parallelism)\n",
        "\n",
        "def get_batch(split):\n",
        "    # Pick a random chunk of data\n",
        "    d = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    \n",
        "    # x is the input, y is the target (the very next character)\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Inputs (x) and Targets (y)\n",
        "\n",
        "We train the model by showing it a sequence and asking it to predict the next character.\n",
        "\n",
        "If the text is \"Hello\":\n",
        "- When input is `[H]`, target is `e`\n",
        "- When input is `[H, e]`, target is `l`\n",
        "- When input is `[H, e, l]`, target is `l`\n",
        "\n",
        "We do this for every position simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # The Embedding layer is a lookup table.\n",
        "        # For every input number, it looks up a list of probabilities for the next number.\n",
        "        self.embed = nn.Embedding(vocab_size, vocab_size)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is the input numbers (Batch, Time)\n",
        "        logits = self.embed(idx)\n",
        "        loss = None\n",
        "        \n",
        "        if targets is not None:\n",
        "            # If we have targets, calculate how wrong we are (loss)\n",
        "            B, T, C = logits.shape\n",
        "            # Reshape to match what PyTorch expects\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new):\n",
        "        # This function generates new text\n",
        "        for _ in range(max_new):\n",
        "            # Get predictions\n",
        "            logits, _ = self(idx)\n",
        "            # Focus only on the last time step (Bigram only cares about the last token)\n",
        "            logits = logits[:, -1, :]\n",
        "            # Convert scores to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample the next character from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Append to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "bigram = BigramModel(vocab_size).to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in bigram.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see what the untrained model generates.\n",
        "# Since it hasn't learned anything, it should just be random gibberish.\n",
        "print(\"Output BEFORE training:\")\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(bigram.generate(context, 100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an optimizer.\n",
        "# The optimizer is the algorithm that updates the model's numbers to reduce the error (loss).\n",
        "optimizer = torch.optim.AdamW(bigram.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Starting training loop...\")\n",
        "for step in range(1000):\n",
        "    # 1. Get a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # 2. Forward pass: Make predictions and calculate loss\n",
        "    _, loss = bigram(xb, yb)\n",
        "    \n",
        "    # 3. Backward pass: Calculate gradients (how to change weights)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # 4. Update weights\n",
        "    optimizer.step()\n",
        "    \n",
        "    if step % 200 == 0: print(f\"Step {step}: loss={loss.item():.4f}\")\n",
        "\n",
        "print(f\"Final loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's generate text again.\n",
        "# It should look slightly better (maybe like real words), but still nonsensical\n",
        "# because it only looks at one character at a time.\n",
        "print(\"Output AFTER training:\")\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(bigram.generate(context, 200)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 1\n",
        "\n",
        "**What we learned:**\n",
        "1.  **Training Loop:** The cycle of Guess -> Measure Error -> Update Weights.\n",
        "2.  **Limitation:** The Bigram model is too simple. It has no \"memory\". It sees \"t\" and guesses \"h\", but it doesn't know if the previous word was \"ca\" (cat) or \"ba\" (bat).\n",
        "\n",
        "**Next Step:** We need a model that can look back at the entire history of words. That is what **Attention** does.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Tokenization\n",
        "\n",
        "In Stage 1, we used characters (a, b, c). But modern models like GPT-4 don't read letters; they read **tokens**.\n",
        "\n",
        "### Why not characters?\n",
        "If we use characters, the model has to learn that 't', 'h', 'e' always go together. That's a waste of time.\n",
        "\n",
        "### Why not whole words?\n",
        "There are too many words in English. The vocabulary would be huge (millions).\n",
        "\n",
        "### The Solution: Byte Pair Encoding (BPE)\n",
        "BPE is a middle ground. It groups common pairs of letters together.\n",
        "- \"ing\" becomes one token.\n",
        "- \"the\" becomes one token.\n",
        "- Rare words like \"Zylophone\" might be split into \"Zy\", \"lo\", \"phone\".\n",
        "\n",
        "We will use `tiktoken`, the exact tokenizer used by OpenAI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Load the tokenizer used by GPT-2 (and GPT-3)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "text = \"Hello, I am learning AI!\"\n",
        "tokens = enc.encode(text)\n",
        "\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"Token IDs: {tokens}\")\n",
        "print(f\"Compression: {len(text)} characters -> {len(tokens)} tokens\")\n",
        "\n",
        "# Let's see what each token represents\n",
        "for t in tokens:\n",
        "    print(f\"  Token {t} -> '{enc.decode([t])}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We wrap the tokenizer in a class to make it easy to use later.\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        self.vocab_size = 50304  # Standard GPT-2 vocabulary size\n",
        "    \n",
        "    def encode(self, text):\n",
        "        # Convert string to list of integers (tensor)\n",
        "        return torch.tensor(self.enc.encode(text), dtype=torch.long)\n",
        "    \n",
        "    def decode(self, tokens):\n",
        "        # Convert list of integers back to string\n",
        "        if isinstance(tokens, torch.Tensor): tokens = tokens.tolist()\n",
        "        return self.enc.decode(tokens)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 2\n",
        "\n",
        "**What we learned:**\n",
        "- **Tokens** are the atoms of language for AI.\n",
        "- **BPE** allows us to represent text efficiently.\n",
        "\n",
        "Now that we have proper data, let's build the brain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 3: Attention (The Heart of Transformers)\n",
        "\n",
        "This is the most important concept in modern AI.\n",
        "\n",
        "**The Problem:** In a sentence like *\"The bank was closed because it was flooded\"*, how does the machine know that \"it\" refers to \"bank\" and not \"closed\"?\n",
        "\n",
        "**The Solution:** **Self-Attention**. Every word looks at every other word and decides how relevant it is.\n",
        "- \"it\" asks: \"What am I?\"\n",
        "- \"bank\" answers: \"I am a noun, and I can be flooded.\"\n",
        "- \"it\" decides: \"Okay, I am referring to the bank.\"\n",
        "\n",
        "We will build this mechanism in 4 steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 1: The Naive Approach (Averaging)\n",
        "\n",
        "The simplest way to get context is to just **average** the features of all previous words.\n",
        "If I am at word 5, I take the average of words 1, 2, 3, 4, and 5.\n",
        "\n",
        "This is weak (it loses order information), but it's a start.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "B, T, C = 4, 8, 2  # Batch=4, Time=8, Channels=2\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# We want x[b,t] to be the average of x[b, 0...t]\n",
        "xbow = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        # Slice all previous tokens and average them\n",
        "        xbow[b, t] = x[b, :t+1].mean(dim=0)\n",
        "\n",
        "print(\"Example row 0 (original):\", x[0,0])\n",
        "print(\"Example row 1 (average of 0 and 1):\", xbow[0,1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 2: Matrix Multiplication (Efficiency)\n",
        "\n",
        "Loops in Python are slow. We can do the exact same averaging using **Matrix Multiplication**.\n",
        "We create a triangular matrix of 1s and normalize it. When we multiply our data by this matrix, it automatically computes the running averages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a lower triangular matrix of 1s\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "# Normalize so rows sum to 1\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "\n",
        "print(\"Weight matrix (notice the lower triangle):\")\n",
        "print(wei)\n",
        "\n",
        "# Perform matrix multiplication\n",
        "xbow2 = wei @ x\n",
        "print(f\"\\nDoes it match the loop version? {torch.allclose(xbow, xbow2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 3: Softmax (Making it smart)\n",
        "\n",
        "Averaging is boring. We want the model to **choose** which past words are important.\n",
        "\n",
        "We use **Softmax**. Softmax takes a list of numbers and turns them into probabilities (they sum to 1).\n",
        "We also use a **Mask** (setting future positions to negative infinity) so the model can't cheat and look at the future.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with zeros (no preference yet)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "\n",
        "# Masking: Set future positions to -infinity\n",
        "# When we take softmax, e^-inf becomes 0. This prevents looking ahead.\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "# Softmax normalizes the weights\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "print(\"Weights after softmax:\")\n",
        "print(wei)\n",
        "\n",
        "xbow3 = wei @ x\n",
        "print(f\"Matches? {torch.allclose(xbow, xbow3)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 4: Self-Attention (The Real Deal)\n",
        "\n",
        "Now for the magic. We don't want uniform averages. We want data-dependent attention.\n",
        "\n",
        "Every token emits three vectors:\n",
        "1.  **Query (Q):** What am I looking for?\n",
        "2.  **Key (K):** What do I contain?\n",
        "3.  **Value (V):** If you find me interesting, here is my information.\n",
        "\n",
        "**The Analogy:**\n",
        "- Imagine a library.\n",
        "- **Query:** You search for \"dinosaurs\".\n",
        "- **Key:** The book titles on the spine.\n",
        "- **Value:** The content inside the book.\n",
        "\n",
        "The attention score is the match between your Query and the book's Key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "head_size = 16\n",
        "\n",
        "# Three linear layers to generate Q, K, V\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# Generate Q, K, V from the input x\n",
        "k, q, v = key(x), query(x), value(x)\n",
        "\n",
        "# Calculate scores: Query dot Key\n",
        "# (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
        "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5)  # Scale by sqrt(d) for stability\n",
        "\n",
        "# Masking (don't look at future)\n",
        "wei = wei.masked_fill(torch.tril(torch.ones(T,T)) == 0, float('-inf'))\n",
        "\n",
        "# Softmax to get probabilities\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# Aggregate values based on attention weights\n",
        "out = wei @ v\n",
        "\n",
        "print(\"Learned attention weights (first batch):\")\n",
        "print(wei[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Attention\n",
        "\n",
        "Let's look at the \"Heatmap\".\n",
        "- The **x-axis** is the token being looked at.\n",
        "- The **y-axis** is the token doing the looking.\n",
        "- **Color intensity** shows how much attention is being paid.\n",
        "\n",
        "Notice the triangle shape? That's the causal mask. You can't look at the future!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize attention weights for batch 0\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Attention heatmap\n",
        "ax1 = axes[0]\n",
        "attn_weights = wei[0].detach().numpy()  # Shape: (T, T)\n",
        "im1 = ax1.imshow(attn_weights, cmap='Blues', aspect='auto')\n",
        "ax1.set_xlabel('Key Position (tokens being attended to)', fontsize=10)\n",
        "ax1.set_ylabel('Query Position (current token)', fontsize=10)\n",
        "ax1.set_title('Attention Heatmap\\n(Causal Mask: can only see past + current)', fontsize=11)\n",
        "ax1.set_xticks(range(T))\n",
        "ax1.set_yticks(range(T))\n",
        "plt.colorbar(im1, ax=ax1, label='Attention Weight')\n",
        "\n",
        "# Add visual markers for masked positions\n",
        "for i in range(T):\n",
        "    for j in range(T):\n",
        "        if j > i:  # Masked (future) positions\n",
        "            ax1.text(j, i, 'X', ha='center', va='center', color='red', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 2: Line plot showing attention distribution per position\n",
        "ax2 = axes[1]\n",
        "colors = plt.cm.viridis([0.2, 0.4, 0.6, 0.8])\n",
        "positions_to_show = [0, 2, 5, 7]\n",
        "for idx, pos in enumerate(positions_to_show):\n",
        "    ax2.plot(range(T), attn_weights[pos, :], 'o-', color=colors[idx], \n",
        "             label=f'Position {pos}', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Key Position (tokens being attended to)', fontsize=10)\n",
        "ax2.set_ylabel('Attention Weight', fontsize=10)\n",
        "ax2.set_title('Attention Distribution per Query Position', fontsize=11)\n",
        "ax2.legend(loc='upper right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xticks(range(T))\n",
        "ax2.set_ylim(-0.05, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key observations:\")\n",
        "print(\"1. Lower-triangular pattern: Can only attend to past + current (causal mask)\")\n",
        "print(\"2. Position 0 attends ONLY to itself (weight = 1.0)\")\n",
        "print(\"3. Later positions can distribute attention across more tokens\")\n",
        "print(\"4. Each row sums to 1.0 (softmax normalization)\")\n",
        "print(f\"\\nRow sums: {[f'{s:.2f}' for s in attn_weights.sum(axis=1)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why do we divide by sqrt(head_size)?\n",
        "\n",
        "This is a technical detail called **Scaling**.\n",
        "If we don't divide, the numbers in the dot product get huge. When numbers are huge, Softmax becomes extremely \"peaky\" (one value is 1, the rest are 0).\n",
        "This kills the gradient (the signal for learning). Scaling keeps the numbers in a nice range so the model can learn smoothly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the scaling problem visually\n",
        "q_demo, k_demo = torch.randn(8, 64), torch.randn(8, 64)\n",
        "raw = q_demo @ k_demo.T\n",
        "scaled = raw * (64 ** -0.5)\n",
        "\n",
        "print(f\"Raw variance: {raw.var():.1f} (grows with dimension!)\")\n",
        "print(f\"Scaled variance: {scaled.var():.1f} (stable around 1.0)\")\n",
        "print()\n",
        "\n",
        "# Visualize the effect on softmax\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Without scaling\n",
        "raw_probs = F.softmax(raw, dim=-1).detach().numpy()\n",
        "im1 = axes[0].imshow(raw_probs, cmap='hot', aspect='auto')\n",
        "axes[0].set_title('WITHOUT Scaling\\n(Peaky - almost one-hot!)', fontsize=11)\n",
        "axes[0].set_xlabel('Key Position')\n",
        "axes[0].set_ylabel('Query Position')\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# With scaling\n",
        "scaled_probs = F.softmax(scaled, dim=-1).detach().numpy()\n",
        "im2 = axes[1].imshow(scaled_probs, cmap='hot', aspect='auto')\n",
        "axes[1].set_title('WITH Scaling (divide by sqrt(d))\\n(Diffuse - can learn!)', fontsize=11)\n",
        "axes[1].set_xlabel('Key Position')\n",
        "axes[1].set_ylabel('Query Position')\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Without scaling: Network is 'overconfident' before learning!\")\n",
        "print(\"With scaling: Attention is diffuse, allowing gradients to flow and learn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 3\n",
        "\n",
        "**What we learned:**\n",
        "- **Self-Attention** allows tokens to talk to each other.\n",
        "- **Q, K, V** is the mechanism: Query matches Key to get Value.\n",
        "- **Masking** ensures we generate text one word at a time without cheating.\n",
        "\n",
        "We now have the brain. Let's upgrade it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 4: Modern Components (Llama 3)\n",
        "\n",
        "The original Transformer was invented in 2017. AI moves fast. We will use the modern improvements found in **Llama 3 (2024)**.\n",
        "\n",
        "1.  **RMSNorm:** A simpler, faster way to normalize numbers.\n",
        "2.  **RoPE:** A better way to handle position (angles instead of addition).\n",
        "3.  **SwiGLU:** A more expressive neuron activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # Learnable scaling parameter\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Calculate the Root Mean Square (RMS)\n",
        "        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        # Normalize and scale\n",
        "        return x * rms * self.weight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 RoPE (Rotary Positional Embeddings)\n",
        "\n",
        "**Old way:** Add a number to the embedding to say \"I am at position 5\".\n",
        "**New way (RoPE):** Rotate the vector by an angle.\n",
        "\n",
        "Think of a clock. Position 1 is 1:00, Position 2 is 2:00. The angle tells you the position. This works much better for long sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precompute_freqs_cis(dim, max_len, theta=10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
        "    t = torch.arange(max_len)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    # xq, xk: (B, T, nh, head_dim)\n",
        "    # freqs_cis: (T, head_dim//2) complex\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    # Broadcast freqs_cis to (1, T, 1, head_dim//2) for multi-head\n",
        "    freqs_cis = freqs_cis[:xq.shape[1]].unsqueeze(0).unsqueeze(2)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 SwiGLU\n",
        "\n",
        "This is the \"neuron\" part of the brain.\n",
        "Standard neurons (ReLU) are simple switches: On or Off.\n",
        "**SwiGLU** is a gated mechanism. It's like a door that can be partially open, controlled by another neuron. It allows for much more complex decision making.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, dim, hidden=None, bias=False):\n",
        "        super().__init__()\n",
        "        hidden = hidden or int(dim * 4 * 2/3)\n",
        "        self.w1 = nn.Linear(dim, hidden, bias=bias)\n",
        "        self.w2 = nn.Linear(hidden, dim, bias=bias)\n",
        "        self.w3 = nn.Linear(dim, hidden, bias=bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 4\n",
        "\n",
        "We have gathered the state-of-the-art parts:\n",
        "- **RMSNorm** for stability.\n",
        "- **RoPE** for position.\n",
        "- **SwiGLU** for thinking.\n",
        "\n",
        "Now, let's put them all together into a full GPT model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 5: Assembling the Full GPT Model\n",
        "\n",
        "We will now build the full architecture.\n",
        "\n",
        "**The Blueprint:**\n",
        "1.  **Embedding:** Convert Token IDs to Vectors.\n",
        "2.  **Transformer Blocks:** A stack of layers (Communication + Computation).\n",
        "    - **Communication:** Attention (Tokens talk to each other).\n",
        "    - **Computation:** FeedForward (Tokens think about what they heard).\n",
        "3.  **Output Head:** Convert vectors back to probabilities for the next token.\n",
        "\n",
        "### Configuration Presets\n",
        "\n",
        "| Mode | Layers | Embed | Params | Memory | Best For |\n",
        "|------|--------|-------|--------|--------|----------|\n",
        "| **COLAB_MODE=True** | 6 | 384 | ~30M | ~2GB | Free Colab T4 |\n",
        "| COLAB_MODE=False | 12 | 768 | ~124M | ~6GB | Better GPU |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "# Configuration flags\n",
        "COLAB_MODE = True  # Use this for free Google Colab T4 GPU\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # block_size: How far back the model can look (context window)\n",
        "    block_size: int = 512 if COLAB_MODE else 1024\n",
        "    # vocab_size: Number of unique tokens\n",
        "    vocab_size: int = 50304\n",
        "    # n_layer: Number of Transformer blocks (depth)\n",
        "    n_layer: int = 6 if COLAB_MODE else 12\n",
        "    # n_head: Number of attention heads (parallel focus)\n",
        "    n_head: int = 6 if COLAB_MODE else 12\n",
        "    # n_embd: Size of the vector representing each token (width)\n",
        "    n_embd: int = 384 if COLAB_MODE else 768\n",
        "    dropout: float = 0.1 if COLAB_MODE else 0.0\n",
        "    bias: bool = False\n",
        "\n",
        "config = GPTConfig()\n",
        "print(f\"Model Configuration:\")\n",
        "print(f\"  - Layers: {config.n_layer}\")\n",
        "print(f\"  - Embedding Size: {config.n_embd}\")\n",
        "print(f\"  - Context Window: {config.block_size}\")\n",
        "print(f\"  - Parameter Count: {'~30M' if COLAB_MODE else '~124M'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        \n",
        "        # Key, Query, Value projections combined into one layer for efficiency\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        # Check if we can use Flash Attention (a super fast optimization)\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "    \n",
        "    def forward(self, x, freqs_cis=None):\n",
        "        B, T, C = x.size()\n",
        "        # Calculate Q, K, V\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        \n",
        "        # Reshape for multi-head attention\n",
        "        # Split the embedding into 'n_head' smaller vectors\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # Apply RoPE (Rotary Positional Embeddings)\n",
        "        if freqs_cis is not None:\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "            q, k = apply_rotary_emb(q, k, freqs_cis)\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "        \n",
        "        # Calculate attention scores\n",
        "        if self.flash:\n",
        "            # Use the optimized PyTorch implementation if available\n",
        "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        else:\n",
        "            # Manual implementation (for understanding)\n",
        "            att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "            att = att.masked_fill(torch.tril(torch.ones(T, T, device=x.device)) == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            y = att @ v\n",
        "        \n",
        "        # Reassemble the heads back into one big vector\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.dropout(self.c_proj(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = RMSNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = RMSNorm(config.n_embd)\n",
        "        self.mlp = SwiGLU(config.n_embd)\n",
        "    \n",
        "    def forward(self, x, freqs_cis):\n",
        "        # Residual Connection: x = x + ...\n",
        "        # This allows the signal to flow through the network easily (like a highway)\n",
        "        x = x + self.attn(self.ln_1(x), freqs_cis)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # The container for all our layers\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd), # Token embeddings\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # The layers\n",
        "            ln_f = RMSNorm(config.n_embd), # Final normalization\n",
        "        ))\n",
        "        # The output head that predicts the next token\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        \n",
        "        # Weight Tying: We use the same weights for input embedding and output prediction.\n",
        "        # This makes sense because if 'cat' has a certain vector input, it should have the same vector output.\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        \n",
        "        # Precompute RoPE frequencies\n",
        "        self.freqs_cis = precompute_freqs_cis(\n",
        "            config.n_embd // config.n_head, config.block_size * 2\n",
        "        ).to(device)\n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, m):\n",
        "        # Initialize weights with small random numbers\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None: torch.nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        \n",
        "        # 1. Convert token IDs to vectors\n",
        "        x = self.transformer.wte(idx)\n",
        "        \n",
        "        # 2. Get position frequencies\n",
        "        freqs_cis = self.freqs_cis[:T]\n",
        "        \n",
        "        # 3. Run through all Transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, freqs_cis)\n",
        "        \n",
        "        # 4. Final normalization\n",
        "        x = self.transformer.ln_f(x)\n",
        "        \n",
        "        # 5. Calculate output (logits) and loss\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        else:\n",
        "            # Inference-time optimization: only compute for the last token\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "        return logits, loss\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new, temperature=1.0, top_k=None):\n",
        "        # Generation loop\n",
        "        for _ in range(max_new):\n",
        "            # Crop context if it gets too long\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # Get prediction\n",
        "            logits, _ = self(idx_cond)\n",
        "            # Scale by temperature (higher = more random, lower = more focused)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # Top-K sampling (optional): only keep the top K most likely options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx = torch.cat((idx, torch.multinomial(probs, 1)), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPT(config).to(device)\n",
        "print(f\"Model instantiated with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters.\")\n",
        "\n",
        "# Compile the model (PyTorch 2.0 optimization) for faster training\n",
        "if hasattr(torch, 'compile'):\n",
        "    print(\"Compiling model...\")\n",
        "    model = torch.compile(model)\n",
        "    print(\"Model compiled.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 5\n",
        "\n",
        "We have built the robot! It has:\n",
        "- **Eyes:** Tokenizer + Embedding\n",
        "- **Brain:** Transformer Blocks (Attention + SwiGLU)\n",
        "- **Mouth:** LM Head\n",
        "\n",
        "But it hasn't gone to school yet. It knows nothing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 6: Training\n",
        "\n",
        "We will train the model on **FineWeb-Edu**, a dataset of high-quality educational content.\n",
        "\n",
        "### Checkpointing Strategy (Important!)\n",
        "If you are running on a free cloud machine, it might disconnect at any time.\n",
        "To save our work, we will:\n",
        "1.  **Save** the model to a local folder named `checkpoints`.\n",
        "2.  **Download** the `ckpt.pt` file to our own computer manually.\n",
        "3.  **Upload** it back if we need to resume training later.\n",
        "\n",
        "### Training Techniques\n",
        "1.  **Mixed Precision:** We use `float16` (half precision) instead of `float32`. It uses half the memory and runs 2x faster with almost no loss in quality.\n",
        "2.  **Gradient Accumulation:** We want to simulate a large batch size (e.g., 32) but our GPU can only fit 4. So we run 4 small batches, add up the gradients, and *then* update the weights.\n",
        "3.  **Cosine Decay:** We start with a high learning rate to learn fast, then slowly lower it to fine-tune the details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What to Expect\n",
        "\n",
        "- **Step 0:** Complete gibberish.\n",
        "- **Step 500:** It starts to spell words correctly.\n",
        "- **Step 2000:** It forms sentences.\n",
        "- **Step 5000:** It writes coherent paragraphs.\n",
        "\n",
        "**Note:** This is a small model trained for a short time. It won't be Einstein, but it will speak English.\n",
        "With 6 layers and 512 context, this is a \"baby\" model.\n",
        "Real ChatGPT has 100+ layers and 100K+ context.\n",
        "The goal here is **understanding**, not production quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Troubleshooting: Out of Memory\n",
        "\n",
        "If your GPU runs out of memory (OOM), try:\n",
        "1.  Restarting the runtime (Runtime -> Restart Runtime).\n",
        "2.  Reducing `batch_size` in the code below.\n",
        "3.  Reducing `block_size` (context window).\n",
        "4. **Clear GPU cache:** Run this cell:\n",
        "\n",
        "```python\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, batch_size, block_size, split='train'):\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        # Stream the dataset from Hugging Face so we don't download 500GB at once\n",
        "        self.dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \n",
        "            name=\"sample-10BT\", split=split, streaming=True)\n",
        "        self.iterator = iter(self.dataset)\n",
        "        self.buffer = []\n",
        "    \n",
        "    def __iter__(self): return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        # Fill buffer with tokens until we have enough for a batch\n",
        "        needed = self.batch_size * self.block_size + 1\n",
        "        while len(self.buffer) < needed:\n",
        "            try:\n",
        "                text = next(self.iterator)['text']\n",
        "                self.buffer.extend(tokenizer.enc.encode(text))\n",
        "            except StopIteration:\n",
        "                self.iterator = iter(self.dataset)\n",
        "        \n",
        "        # Take a chunk\n",
        "        chunk = self.buffer[:needed]\n",
        "        self.buffer = self.buffer[needed:]\n",
        "        \n",
        "        # Create tensors\n",
        "        data = torch.tensor(chunk, dtype=torch.long)\n",
        "        x = data[:-1].view(self.batch_size, self.block_size)\n",
        "        y = data[1:].view(self.batch_size, self.block_size)\n",
        "        return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Settings\n",
        "max_iters = 5000              # Total training steps\n",
        "warmup_iters = 200            # Warmup steps (start slow)\n",
        "eval_interval = 250           # How often to check progress\n",
        "save_interval = 500           # How often to save to Google Drive\n",
        "batch_size = 4 if COLAB_MODE else 8\n",
        "grad_accum_steps = 4          # Accumulate gradients to simulate larger batch\n",
        "max_lr = 3e-4                 # Maximum learning rate\n",
        "min_lr = 1e-5                 # Minimum learning rate\n",
        "\n",
        "# Learning rate schedule\n",
        "def get_lr(it):\n",
        "    # 1. Linear Warmup\n",
        "    if it < warmup_iters:\n",
        "        return max_lr * (it + 1) / warmup_iters\n",
        "    # 2. Cosine Decay\n",
        "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "print(\"Training Configuration Ready.\")\n",
        "print(f\"  - Max iterations: {max_iters:,}\")\n",
        "print(f\"  - Batch size: {batch_size} × {grad_accum_steps} = {batch_size * grad_accum_steps} effective\")\n",
        "print(f\"  - Tokens per step: {batch_size * grad_accum_steps * config.block_size:,}\")\n",
        "print(f\"  - LR: {min_lr} → {max_lr} → {min_lr} (warmup + cosine)\")\n",
        "print(f\"  - Checkpointing every {save_interval} steps to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to Resume Training\n",
        "\n",
        "If you have a `ckpt.pt` file from a previous session on your computer:\n",
        "1.  Run the cell below.\n",
        "2.  Click **\"Choose Files\"**.\n",
        "3.  Select your `ckpt.pt` file.\n",
        "4.  The code will automatically move it to the correct folder so training can resume.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this ONLY if you want to resume from a saved checkpoint file.\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"Upload your 'ckpt.pt' file here (if you have one)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    # Move the uploaded file to our checkpoint directory\n",
        "    destination = os.path.join(PROJECT_DIR, \"ckpt.pt\")\n",
        "    shutil.move(filename, destination)\n",
        "    print(f\"Restored checkpoint to {destination}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(batch_size, config.block_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "scaler = torch.amp.GradScaler('cuda')  # For mixed precision\n",
        "\n",
        "# Checkpoint logic: Resume if file exists in our local folder\n",
        "CKPT = os.path.join(PROJECT_DIR, \"ckpt.pt\")\n",
        "start_iter = 0\n",
        "if os.path.exists(CKPT):\n",
        "    print(f\"Found checkpoint at {CKPT}. Resuming...\")\n",
        "    ckpt = torch.load(CKPT, map_location=device, weights_only=False)\n",
        "    state_dict = ckpt['model']\n",
        "    # Fix for compiled model names\n",
        "    new_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
        "    model.load_state_dict(new_state_dict, strict=False)\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    start_iter = ckpt['iter'] + 1\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting fresh.\")\n",
        "\n",
        "losses = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training loop...\")\n",
        "model.train()\n",
        "t0 = time.time()\n",
        "running_loss = 0.0\n",
        "\n",
        "for it in range(start_iter, max_iters):\n",
        "    # Set learning rate\n",
        "    lr = get_lr(it)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    \n",
        "    # Gradient Accumulation\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        xb, yb = next(train_loader)\n",
        "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
        "            logits, loss = model(xb, yb)\n",
        "            loss = loss / grad_accum_steps\n",
        "        scaler.scale(loss).backward()\n",
        "    \n",
        "    # Clip gradients (prevent instability)\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    \n",
        "    # Update weights\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    \n",
        "    running_loss += loss.item() * grad_accum_steps\n",
        "    \n",
        "    # Logging\n",
        "    if it % 10 == 0:\n",
        "        avg_loss = running_loss / 10 if it > 0 else running_loss\n",
        "        losses.append(avg_loss)\n",
        "        running_loss = 0.0\n",
        "        dt = time.time() - t0; t0 = time.time()\n",
        "        print(f\"Step {it:5d}/{max_iters} | loss={avg_loss:.4f} | lr={lr:.2e} | {dt*1000:.0f}ms\")\n",
        "    \n",
        "    # Plotting\n",
        "    if it % eval_interval == 0 and it > 0:\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('Step (x10)')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Training Progress - Step {it}/{max_iters}')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    \n",
        "    # Saving\n",
        "    if it % save_interval == 0 and it > 0:\n",
        "        raw_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "        torch.save({\n",
        "            'model': raw_model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'iter': it,\n",
        "            'config': config,\n",
        "            'losses': losses\n",
        "        }, CKPT)\n",
        "        print(f\"Checkpoint saved to {CKPT}\")\n",
        "        print(\"Remember to download this file if you want to keep it!\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to DOWNLOAD your trained model to your computer.\n",
        "from google.colab import files\n",
        "\n",
        "if os.path.exists(CKPT):\n",
        "    print(f\"Downloading {CKPT}...\")\n",
        "    files.download(CKPT)\n",
        "else:\n",
        "    print(\"No checkpoint found to download!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 6\n",
        "\n",
        "We have successfully trained the model. It has seen millions of tokens and adjusted its weights to predict the next word.\n",
        "\n",
        "Now comes the fun part: testing it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 7: Inference & Chat\n",
        "\n",
        "We will now switch the model to **Evaluation Mode** and interact with it.\n",
        "We use a `temperature` parameter to control creativity:\n",
        "- **Low Temperature (0.1):** Focused, deterministic, boring.\n",
        "- **High Temperature (1.0):** Creative, random, sometimes crazy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(max_tokens=100, temp=0.8):\n",
        "    model.eval()  # Turn off dropout for consistent results\n",
        "    print(\"Chat Interface (Type 'exit' to stop)\")\n",
        "    print(\"-\" * 40)\n",
        "    while True:\n",
        "        prompt = input(\"You: \")\n",
        "        if prompt.lower() == 'exit': break\n",
        "        \n",
        "        # Encode prompt\n",
        "        idx = tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
        "        print(\"AI: \", end=\"\", flush=True)\n",
        "        \n",
        "        # Generate tokens one by one\n",
        "        for _ in range(max_tokens):\n",
        "            out = model.generate(idx, max_new=1, temperature=temp)\n",
        "            new_tok = out[0, -1].item()\n",
        "            print(tokenizer.decode([new_tok]), end=\"\", flush=True)\n",
        "            idx = out\n",
        "        print(\"\\n\" + \"-\" * 40)\n",
        "\n",
        "# Uncomment the line below to run the chat\n",
        "# chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 7\n",
        "\n",
        "You now have a working AI chatbot running on your own code.\n",
        "However, you might notice it just completes sentences rather than answering questions helpfully. That's because it is a **Base Model**.\n",
        "\n",
        "To make it an **Assistant**, we need Alignment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 8: RLHF Alignment (Conceptual)\n",
        "\n",
        "**Reinforcement Learning from Human Feedback (RLHF)** is how we turn a text completer into a helpful assistant.\n",
        "\n",
        "**The Process:**\n",
        "1.  **SFT (Supervised Fine-Tuning):** Train on Q&A datasets.\n",
        "2.  **Reward Modeling:** Train a judge model to rate answers.\n",
        "3.  **PPO (Proximal Policy Optimization):** Use the judge to train the AI to get high scores.\n",
        "\n",
        "Below is a simple demonstration of the **Reward** concept.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_reward(text):\n",
        "    \"\"\"A dummy reward function. Real ones are complex neural networks.\"\"\"\n",
        "    # We reward the model for using positive words\n",
        "    positive = [\"happy\", \"good\", \"great\", \"excellent\", \"love\", \"wonderful\", \"amazing\"]\n",
        "    return sum(1 for w in positive if w in text.lower())\n",
        "\n",
        "# RLHF Demonstration - CONCEPTUAL ONLY (no learning occurs!)\n",
        "# Missing for true RLHF: log_probs collection, value function, PPO loss, gradient update\n",
        "# This demo only shows: sampling/scoring loop (the reward signal concept)\n",
        "print(\"Initializing RLHF demonstration...\")\n",
        "prompts = [\"Today I feel\", \"The weather is\", \"My work is\"]\n",
        "\n",
        "for i in range(50):\n",
        "    prompt = prompts[i % len(prompts)]\n",
        "    idx = tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
        "    \n",
        "    # 1. Generate multiple samples\n",
        "    responses = []\n",
        "    for _ in range(4):\n",
        "        out = model.generate(idx, max_new=10, temperature=0.9)\n",
        "        responses.append(tokenizer.decode(out[0].tolist()))\n",
        "    \n",
        "    # 2. Score them\n",
        "    rewards = [get_reward(r) for r in responses]\n",
        "    avg_reward = sum(rewards) / len(rewards)\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(f\"Step {i}: Average Reward = {avg_reward:.2f}\")\n",
        "        best_idx = rewards.index(max(rewards))\n",
        "        print(f\"  Best Response: {responses[best_idx]}\")\n",
        "\n",
        "print(\"RLHF demonstration complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section Summary: Stage 8\n",
        "\n",
        "In a real scenario, we would use these rewards to update the model's weights using PPO. This encourages the model to generate more \"positive\" (high reward) text in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Glossary\n",
        "\n",
        "| Term | Definition |\n",
        "|------|------------|\n",
        "| **BPE** | Byte Pair Encoding. The method for chopping text into tokens. |\n",
        "| **Logits** | The raw scores output by the model before Softmax. |\n",
        "| **Softmax** | A math function that turns scores into probabilities (0 to 1). |\n",
        "| **Temperature** | A setting that controls how random the model's creativity is. |\n",
        "| **Epoch** | One full pass through the entire training dataset. |\n",
        "| **Inference** | Using the trained model to generate text. |\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You have successfully implemented a complete GPT architecture with modern enhancements.\n",
        "\n",
        "**Key Concepts Covered:**\n",
        "- Byte Pair Encoding (BPE)\n",
        "- Causal Self-Attention and Multi-Head Attention\n",
        "- RMSNorm, RoPE, and SwiGLU\n",
        "- Mixed-precision training and gradient accumulation\n",
        "\n",
        "**Next steps:**\n",
        "1. Train longer (100k+ steps)\n",
        "2. Use larger/better datasets\n",
        "3. Fine-tune on dialogue for assistant behavior\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
